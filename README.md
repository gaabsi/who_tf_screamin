# who_tf_screamin

![Python](https://img.shields.io/badge/python-3.9-blue?logo=python&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-2.8-EE4C2C?logo=pytorch&logoColor=white)
![Lightning](https://img.shields.io/badge/Lightning-2.6-792EE5?logo=lightning&logoColor=white)
![HuggingFace](https://img.shields.io/badge/ğŸ¤—%20HuggingFace-yellow)
![Docker](https://img.shields.io/badge/Docker-ready-2496ED?logo=docker&logoColor=white)

## Description 
Le but du projet est de faire un classifieur qui permet de reconnaitre un pokÃ©mon par son cri.  
Le modÃ¨le utilisÃ© pour la classification est un *AST (Audio Spectrogram Transformer)*, ce type de modÃ¨le est Ã©quivalent a un VIT (Vision Transformer) sur des audios.  
L'audio est transformÃ©e en image (spectrogramme) et un Vision Transformers est appliquÃ© aprÃ¨s.  
Pour Ã©viter d'avoir a rÃ©entrainer un modele from scratch j'ai rÃ©cupÃ©rÃ© un modele deja prÃ© entrainÃ© et seulement les dernieres couches servant a la classification on Ã©tÃ© dÃ©-freeze pour l'adapter Ã  notre tÃ¢che de classification.  

## Demonstration
DÃ©monstration sur le cri d'aquali (vous pouvez verifier il est dans le test set, l'original a jamais Ã©tÃ© vu par le modele)

<p align="center">
  <a href="https://youtu.be/LO-0n_fZ2A8">
    <img src="https://img.youtube.com/vi/LO-0n_fZ2A8/0.jpg" alt="DÃ©mo Who TF Screamin" width="600">
  </a>
</p>

## Pipeline rÃ©sumÃ©e

### Webscraping 
La premiere partie du projet consiste a rÃ©cuperer les donnÃ©es dont nous aurons besoin.  
Il n'existait pas de dataset de cris de pokÃ©mon dÃ©jÃ  pret sur kaggle (du moins pas au moment oÃ¹ j'ai eu l'idÃ©e de ce projet) il a donc fallut construire cette base.  
Pour ce faire, un site nommÃ© pokebip recense toutes les donnÃ©es de tous les pokÃ©mons gÃ©nÃ©ration par gÃ©nÃ©ration (images, stats, cris, moovepool, ...)  
La structure du site Ã©tant trÃ¨s simple il a Ã©tÃ© trÃ¨s facile d'en extraire les donnÃ©es interessantes et d'obtenir ce qu'il nous fallait.  
A l'issue de cette phase on a donc : 
- un pokÃ©dex custom sur la gÃ©nÃ©ration de notre choix (la premiere pour nous) qui contient toutes les info (stats, numÃ©ro dans le dex, url de la page du pokÃ©mon, ...)
- un directory qui contient 151 pokÃ©mons (le nombre de pokemons dans le pokedex la 1G) et dans chaque directory se trouve le cri du pokemon au format .mp3.  

Donc ici on a toutes nos donnÃ©es initiales mais avec un seul Ã©chantillon par pokÃ©mon c'est impossible d'entrainer un classifieur donc on en a pas encore fini avec ces cris.  
La solution puisqu'on peut pas rÃ©cuperer enormement d'echantillons de cri de pokemons Ã§a va etre de les fabriquer nous meme en crÃ©ant artificiellement des variantes qui pourraient vraiment exister d'un meme cri, on dÃ©taillera Ã§a plus tard.

![scraping image](/img/scraping.png)

### Battle Theme Scraping 
Ã‡a va commencer a spoil la partie suivante mais pour crÃ©er des echantillons rÃ©alistes des cris de pokemons, on peut ajouter un bruit de fond a notre cri pour simuler la musique d'ambiance qui se lance lors d'un combat pokÃ©mon.  
Ã‡a nous permet a la fois de faire des cris qui sont rÃ©alistes car ces variantes peuvent vraiment se produire et d'avoir plus d'Ã©chantillons.  
Donc pour ce faire, j'ai trouvÃ© une playlist youtube qui recensait toutes les musiques de la premiere generation et j'ai tÃ©lÃ©chargÃ© les musiques de combat pokÃ©mon uniquement pour qu'on puisse Ã©ventuellement les ajouter a notre augmentation audio future.  

![battle theme scraping image](/img/battle_theme_scraping.png)

### Audio Augmentation
Ã‡a fait dÃ©jÃ  2 parties qu'on le tease, Ã§a y est on en parle !  
Donc l'objectif est de passer de 1 cri a au moins une vingtaine par pokÃ©mon pour que notre classifieur puisse apprendre a reconnaitre ces cris.  
Alors comment on fait pour crÃ©er artificiellement des audios ? Toutes les mÃ©thodes ont Ã©tÃ© expliquÃ©es dans le notebook d'exploration : 
- On joue sur la durÃ©e : un cri qui se joue plus ou moins vite (le facteur a Ã©tÃ© choisi pour ne pas dÃ©naturer le cri et produire un cri plausible)
- On joue sur le pitch : on fait un cri plus aigu ou plus grave de quelques demi-tons (encore une fois de maniÃ¨re modÃ©rÃ©e)
- On passe un filtre High ou Low Pass : on rend le cri plus Ã©touffÃ© ou plus strident (toujours modÃ©rÃ©ment)
- On ajoute un bruit parasite : on dÃ©coupe une partie alÃ©atoire de la musique de bataille et on l'ajoute au cri de base en jouant sur le rapport entre les deux. 

Une fois qu'on a dÃ©fini toutes nos augmentations probables on prend un cri on l'envoie dans la boite magique, il tire alÃ©atoirement 1 ou 4 augmentations qui sont elles memes alÃ©atoires.  
On itÃ¨re jusqu'a avoir autant de cris qu'on veut par pokÃ©mon et dans notre cas on a pris 25 cris par pokÃ©mon.  

![audio augmentation image](/img/audio_augmentation.png)

### Preprocessing
Une fois qu'on a tous nos cris, le but est d'uniformiser tous ces cris.  
- On prend donc l'audio le plus long qu'on a et on l'utilise comme rÃ©fÃ©rence, tous les audios qui sont plus courts que celui la on rajoutera des 0 au debut et a la fin (on garde le cri au milieu).
- Si notre audio est en stÃ©rÃ©o on le passe en mono.
- On resample nos audios pour passer Ã  16.000 Hz (quasi aucun changement) mais on le fait nous meme plutot que de le laisser au modÃ¨le qui lui aurait fait un carnage Ã  nos audios.  

Donc on applique toutes ces transformations Ã  tous nos audios pour les standardiser.  
Une fois qu'on a fait cela on dÃ©coupe nos donnÃ©es en 3 set (train, val et test) on le fait nous meme encore une fois pour pouvoir gÃ©rer finement ce processus et on se retrouve avec 3 dossiers qui contiennent nos audios scrapÃ©s, prÃ©-processÃ©s et maintenant splitÃ©s.  
Pour les tailles de ces diffÃ©rents split on est Ã  70% de train, 20% de validation et 10% de test.  

![preprocessing image](/img/preprocessing.png)

### Modelisation
Pour la partie modÃ©lisation, on a pris un type de modele qui marche super bien pour la classification audio : un AST (Audio Spectrogram Transformer), pour ne pas avoir a en entrainer un from scratch et capitaliser sur le travail qui a dÃ©jÃ  Ã©tÃ© effectuÃ© on a rÃ©utilisÃ© un AST fait par le MIT dont on a gardÃ© le backbone (tout le modele sauf la tete de classification) et on a juste rÃ©entrainÃ© la tete qui sert a predire la classe en sortie de modele.  
On garde donc les benefices de l'entrainement lourd du modÃ¨le (reconnaissance de patterns, ... ) et on le spÃ©cialise a notre tache de classification particuliere (transfer learning). 

![modelisation image](/img/modelisation.png)

### Evaluation
Pour Ã©valuer notre modele on commence par regarder si nos performances sur le jeu de val suivent la meme trajectoire que celles sur le jeu de train.  
C'est notre cas, la loss et l'accuracy sont au coude a coude sur les 2 jeux donc notre modele n'a pas sur-appris.  
On voit que le gain marginal d'une epoch devient de plus en plus nÃ©gligeable a partir de la 7-8e epoch mais ne stagne pas et ne dÃ©clanche pas non plus notre earlystopping donc on aurait meme pu continuer encore un peu le training si on voulait.  
Globalement notre modele semble avoir d'excellentes performances alors qu'on le rappelle on a au total que 25 audios par classe (dont 3 dont le modele n'a encore pas connaissance).  
On passe donc a la phase de prÃ©diction sur les nouvelles donnÃ©es (celles de test) et on obtient un score de 96% de prÃ©cision sur ce jeu de donnÃ©es ce qui colle avec les performances qu'on pouvait voir sur les courbes mentionnÃ©es prÃ©cÃ©demment.  
Globalement on a un modele trÃ¨s robuste et qui a d'excellentes perfs (gg Ã  moi).  

![evaluation image](/img/evaluation.png)

## Structure du projet 
```
who_tf_screamin/
    â”œâ”€â”€ README.md 
    â”œâ”€â”€ explo.ipynb                     # Notebook d'exploration dans lequel je dÃ©finis les regles d'augmentation et prend en main torchaudio
    â”œâ”€â”€ src/                            # Codes du projet
    â”‚   â”œâ”€â”€ scraping.py                 # WebScraping du pokedex et de tous les cris des pokÃ©mons de la gÃ©nÃ©ration cible
    â”‚   â”œâ”€â”€ music_theme_scraping.py     # Scraping des musiques de combat sur youtube
    â”‚   â”œâ”€â”€ scream_even_more.py         # Augmentation audio (gÃ©nÃ©ration artificielle de cris probables) de 1 Ã  25 cris par pokÃ©mon
    â”‚   â”œâ”€â”€ preprocessing.py            # Preprocessing audio (standardisation, split, ...)
    â”‚   â”œâ”€â”€ modele.py                   # Modelisation et entrainement
    â”‚   â”œâ”€â”€ evaluation.py               # Evaluation des performances du modele
    â”‚   â””â”€â”€ app.py                      # Streamlit ultra basique pour me la peter dans le README
    â”œâ”€â”€ models/                         # Stockage des modeles entrainÃ©s
    â”‚   â””â”€â”€ version_0/                  # Premiere (et unique) version du modele
    â”‚       â”œâ”€â”€ version_O.ckpt          # Poids de notre premier modele (non plus mais sur Hugging Face Hub on le rÃ©cupÃ¨re juste aprÃ¨s)
    â”‚       â””â”€â”€ lightning_logs/         # Logs de l'entrainement du modele
    â”‚           â”œâ”€â”€ eval_modele.png     # SchÃ©mas pour monitorer l'eventuel l'over/under fit du modele
    â”‚           â”œâ”€â”€ metrics.csv         # Logs gÃ©nÃ©rÃ©s par lightning
    â”‚           â””â”€â”€ test_perf.csv       # PrÃ©dictions dÃ©taillÃ©es (chaque pred sur le test set, sa proba, son true label, ...)
    â”œâ”€â”€ data/                           # Data dir (non push) : il est initialement scrapÃ© puis successivement augmentÃ© processÃ© et splitÃ© 
    â”‚   â”œâ”€â”€ split_data/                 # Dir qui contient les train, val et test data splitÃ©s proprement 
    â”‚   â”‚   â”œâ”€â”€ train/                  # Audios utilisÃ©s pour le training
    â”‚   â”‚   â”œâ”€â”€ val/                    # Audios utilisÃ©s pour la validation
    â”‚   â”‚   â””â”€â”€ test/                   # Audios utilisÃ©s pour le test
    â”‚   â”œâ”€â”€ screams/                    # Cris de base, scrapÃ©s directement, sans aucune modification
    â”‚   â”œâ”€â”€ battle_themes/              # Musiques de combats qu'on a scrapÃ©s
    â”‚   â””â”€â”€ pokedex.csv                 # Pokedex de la gÃ©nÃ©ration de pokÃ©mon qu'on a voulu scraper
    â”œâ”€â”€ img/                            # Schemas pour le cotÃ© waouw du README  
    â”‚   â”œâ”€â”€ schema1.png
    â”‚   â””â”€â”€ ... 
    â”œâ”€â”€ Dockerfile                      # Dockerfile du projet
    â”œâ”€â”€ main.sh                         # Pour lancer tout le projet sans se casser la tete (entrainement gourmand, dÃ©conseillÃ© en local, prÃ©ferer une VM)
    â”œâ”€â”€ .gitignore                      # Fichiers a ignorer par Git 
    â””â”€â”€ requirements.txt                # Packages du projet
```

## Clonage du projet 
Pour reproduire le projet et obtenir ses propres poids je conseille vivement de lancer ce dernier soit sur une machine avec un bon GPU soit d'en louer un (j'ai personnellement utilisÃ© un pod de runpod avec une A40 qui coute 0.4$ de l'heure) que j'ai build a partir de mon image Docker dans Docker Hub, le temps d'entrainement du modele est d'environ 20mn.  
On peut s'y connecter via SSH, sortir les logs et les checkpoints soit via FTP soit en tÃ©lÃ©chargeant en clique bouton via l'UI de JupyterLab. 

Sinon pour juste faire tourner le modele en local en utilisant mes poids c'est beaucoup plus rapide et Ã§a tourne sans probleme en local dans un conteneur.  

Pour ce deuxieme cas on peut le faire rapidement comme Ã§a : 

```bash 
cd ~
git clone https://github.com/gaabsi/who_tf_screamin.git
```

Maintenant on rÃ©cupere les poids du modÃ¨le qu'on a deja entrainÃ© : 

```bash 
cd ~/who_tf_screamin
huggingface-cli download gaabsi/who_tf_screamin version_0.ckpt --local-dir ./models/version_0
rm -rf ./models/version_0/.cache/
```

On crÃ©e l'image Docker et on run : 

```bash 
cd ~/who_tf_screamin
docker buildx build --platform linux/amd64,linux/arm64 -t who_tf_screamin:latest . 
docker run --rm -v ~/who_tf_screamin:/app who_tf_screamin:latest  
```

Pour le premier cas (faire tourner le projet entier soi meme) il faudra modifier le main.sh et dÃ©-commenter les lignes de training.  
Mais vraiment il faut un bon GPU pour le run en local. 